{
  "id": "68796c78a4ac7ef2a17a6188",
  "lessonNumber": 7,
  "title": "Lesson 07 — Data Cleaning and Validation with Pandas",
  "status": "pending",
  "assignment": {
    "title": "Assignment for Lesson 7",
    "objective": "No objective specified",
    "expectedCapabilities": [],
    "instructions": [],
    "tasks": [
      {
        "taskNumber": 1,
        "title": "Task 1",
        "description": "# Advanced Data Cleaning and Validation Assignment\n\n### Data Cleaning and Validation with Pandas\n\n### **Objective:**\n\nThis assignment is an approximation of a real world data cleaning case.  You have four copies of a dataset, each with errors.  You want to create a single authoritative dataset that cleans all the errors out.  In addition, you'll get practice in using regular expressions for data transformation.\n\nThis assignment is to be created in a Kaggle notebook, as you did for Assignments 5 and 6.  This time, create a notebook called CTD_Assignment_7 for code as described below.  As for all assignments that use notebooks, you should create a markdown cell above your code that marks the location where you completed each coding task.\n\nComplete the tasks below to demonstrate your understanding of data cleaning and validation techniques. Submit your code and outputs for each task.\n\n---\nThis assignment asks you to do some fairly serious data cleaning.  You will start from 4 CSV files.  Each of the files describes 400 people, with a name, address, zip, and phone number for each.  However:\n1. Some of the people have the same name.\n\n2. There are only 200 different addresses.  Some people share a house.\n\n3. Of the people that share addresses, 20% also share a phone number.\n\n4. Each of the CSV files record this information, **but there are errors in each of the files.** The errors are different in each of the files.\n\n5. In 15% of the records, the phone number is missing.\n\n6. In another 15% of the records, the phone number is incorrect.\n\n7. In 20% of the records, the zipcode is missing.\n\n8. In 15% of the records, the name is misspelled.\n\n9. In 15% of the records, the address is misspelled.\n\n10. Some rows may have more than one error.\n\nYour task is to create a dataset with 400 rows, one for each of the people, that corrects these errors by comparing the versions in each of the files.\n\n## Task 1: How Would You Solve This?\n\n1. Think: How would you do this, using the power of Pandas?\n\n2. Create a markdown cell in your notebook.  Write down a list of 5 or so ideas about how you could do this.  You do not need to use markdown -- plain text will suffice.\n\n\n## Task 2: Load Dataset ##\n\n1. In your notebook, click on the \"Add Input\" button in the upper left, and then on \"Datasets\".  Do a search on \"Code The Dream Assignment 6\", and then on the plus sign next to that dataset to add it to your notebook.  Run the first cell of the notebook to resolve the file names.\n\n2. Create a code cell.  Within it, import pandas and load the four CSV files into four DataFrames.  Print out the first 5 lines of one of these DataFrames, so that you understand the data shape.  Each of the datasets is similar.\n\n3. You need an additional package, called \"thefuzz\".  This package does approximate matching of text strings.  However, it is not part of the standard library delivered with Kaggle, so you need to add this code:\n\n```python\ntry: \n    from thefuzz import process \nexcept ImportError: \n    !pip install thefuzz \n    from thefuzz import process\n```\n4. Concatenate all 4 DataFrames into one, call it `df`.  Print out the info for df.  You should have 1600 rows, but there are NaN values and other problems.\n\n## Task 3: Clean Up Spelling Errors\n\nWe know that some of the names are misspelled.  We are going to assume that 3 of the spellings are correct, and that the remainder is bad.  Of course, several people have the same name, and some people have the same address, so that for a given name, or a given address, there may be more than 4 rows.  But, we'll assume that if a name or address is spelled the same way 3 or more times, that's the correct spelling.  (This assumption may well be false -- but, as we'll see, it still helps to fix up the majority of cases)\n\n1. Find the list of probably correct names.  You use the value_counts() method, which returns a Series.  In the Series, the index is the list of names, and the values are the count for each.  As follows:\n   ```python\n   df_names = df.value_counts('Name')\n   names = list(df_names[df_names > 2].index)\n   ```\n   Print out the first 10 entries of the list of names.\n\n2. Ok, now we have the list of probably good names.  What do we do about the bad ones?  We want to replace each with the good name that is most similar to it, using the `thefuzz` package. As follows:\n   ```python\n   df['Name'] = df['Name'].map(lambda x : x if x in names else process.extractOne(x, names)[0])\n   ```\n   Here, process.extractOne() goes through the list of good names to find the best match.  It returns a tuple, where the first element is the match, and the second is a number that indicates how good the match is.\n\n3. Fix the addresses the same way.\n\n## Task 4: Clean Up the Zip and Phone Columns\n\nWe now have four versions of the information for each person.  We can compare these and, one would hope, we can find the right values, basically by collecting the votes from each.  This is a new technique, so the answer is as follows:\n\n```python\ndef fix_anomaly(group): \n    group_na = group.dropna() \n    if group_na.empty: \n        return group.values \n    mode = group_na.mode() \n    if mode.empty: \n        return group.values \n    return mode.iloc[0]\n\ndf['Zip'] = df.groupby(['Name', 'Address'], as_index=False)['Zip'].transform(fix_anomaly).reset_index(drop=True)\n```\nLet's explain this code.  \n- We do a groupby() for name and address.  The records where the both the name and address are the same are for the same person.  \n- We use as_index=False because we don't want to change the indexing.  \n- Then we do a transform() on the Zip column.  This works kind of like the map() function, but you are passed each group in turn.  \n- Each time we get a group of values, we collect the votes.  First we do a dropna(), because a NaN value doesn't get a vote.  Then we use the mode() function to find the most common value, and change the value for the entire group to match.  \n- There may be several modes, for example if there are 2 of one value and 2 of another, so we just take the first mode. (Actually this is a little dangerous, because one of the other modes might be the correct one.  The correct approach might be to leave the values unchanged if there is more than one mode.)  \n- We have to handle a couple of edge cases.  First, all the values may be NaN, in which case we have nothing to do.  Second, there may be no mode.  This happens if each of the non-null values occurs once.  So then we just leave the values unchanged, because we don't know which one to use. After the transform() completes, we are done with the groupby(), so we reset the index.\n\n1. Fix the phone number column the same way.  Hint: You can reuse fix_anomaly().\n\n## Task 5: The Final Consolidation\n\n1. Eliminate duplicate records.\n\n2. Print out the first 10 records of the resulting DataFrame.\n\n3. Print out the info for the end DataFrame.\n\nHmm.  413 records reported by info().  We were hoping for 400 authoritative records.  And, we still have some records with null values.  So, next step:\n\n## Task 6: Failure Analysis\n\nValidation of the final result, and failure analysis for whatever isn't working right, is a critical part of data cleaning.\n\n1. Print out all the rows that have null values.  (Note: This only finds some of the errors, as there are still erroneous records that have no null values.)\n\n2. We can now subset the original data to figure out what went wrong.  So, add a line to save a copy of the df dataframe, as it was right after the concatenation.  Call the copy df_save.\n\n3. In the records that have null values, you see one for \"Tammie ThoXXmas\".  This looks like a misspelling that should have been corrected.  Print out all the rows from df_save that have the name \"Tammie ThoXXmas\".  Can you see what is wrong?\n\n4. Here's the answer: There are two people named Tammie Thomas, one in Minnesota, one in Massachusetts.  So the original dataset had 8 entries for the two people with that name.  And, in three of them, the name is misspelled in exactly the same way.  Can you see why this would cause trouble for our code?  This is exactly the sort of thing that happens with real-world data.\n\n5. We see that there is also a problem for \"Charles Smith\".  Print out all the rows from df_save that have this name.\n\n## Task 7: How to Improve\n\n1. Create another markdown cell.  Explain what happened for \"Tammie ThoXXmas\", so that this name was not corrected.\n\n2. Also, in the markdown cell, explain why the approach failed for \"Charles Smith\".\n\n3. What could be done to make the data valid?  Put your ideas into the markdown cell.  Hint: The solution is often not more code.\n\n4. The assumption that a name is not correct unless it is spelled the same way 3 or more times has introduced problems.  \"Mrs. Gail Perez\" was misspelled twice, so all 4 instances were replaced by \"Mrs. Taylor Johnson DDS\".  That's dead wrong. The name and address fixing algorithm is really not very good. How could we avoid errors like this?  Also, since this assignment was created, someone thought of a check you can perform to find nearly all errors, but it must be done before you remove duplicates.  What is that check?\n\nYou see that data cleaning often involves assumptions.  Those require careful thought. Such assumptions are necessary, but they might be wrong.\n\n## Task 8: Regular Expressions for Data Cleaning\n\n1. Given the following data, use the `Series.str.extract` method to create a `DataFrame` with columns for 'timestamp', 'level', and 'message'.  Assign it to the variable `extracted_logs` and print it.  Note that each capture group will create a separate column in the `DataFrame` returned by the `extract` method.\n\n```python\nlog_entries = pd.Series([\n    \"[2023-10-26 10:00:00] INFO: User logged in\",\n    \"[2023-10-26 10:05:30] WARNING: Invalid input\",\n    \"[2023-10-26 10:10:15] ERROR: Database connection failed\",\n    \"[2023-10-26 10:12:45] DEBUG: Processing request\"\n])\n```\n\n2. Given the following `Series` change all of the placeholders to the string `<VALUE>` using the `replace` method.  Save the result in the variable `standardized_text` and print it.  Special characters in the pattern will need to be escaped.\n\n```python\ntext_data = pd.Series([\n    \"Value is {amount}.\",\n    \"The price is [value].\",\n    \"Cost: (number)\",\n    \"Quantity = <qty>\"\n])\n```\n\n3. Given the data provided below, use the `DataFrame.filter` method to select columns ending in `_at`.  Save the result in a variable called `time_columns` and print it.\n\n```python\ndf = pd.DataFrame({\n    \"order_id\": [123, 124, 125, 126],\n    \"customer_name\": [\"Alice\", \"Bob\", \"Charlie\", \"Diana\"],\n    \"order_status\": [\"shipped\", \"cancelled\", \"shipped\", \"delivered\"],\n    \"created_at\": [\"2021-01-05\", \"2021-01-06\", \"2021-01-06\", \"2021-01-07\"],\n    \"updated_at\": [\"2021-01-07\", \"2021-01-07\", \"2021-01-08\", \"2021-01-08\"]\n})\n```\n\n4. Given the provided data, use the `Series.str.contains` method to create a subset for orders which have shipped.  You can us `case=False` to make matches case-insensitive.  Save it in the variable `shipped_orders` and print it.\n\n```python\norder_data = [\n    \"Order #123 has been shipped on 2021-01-05 (Tuesday)\",\n    \"Order #124 has been cancelled\",\n    \"shipment confirmation #125 on 02/06/2021\",\n    \"Order #126 delivered on 01 07 2021\",\n    \"Canceled order #127, refund pending\",\n    \"order #128 - Shipped 2021/03/10\"\n]\norders = pd.Series(order_data)\n```\n\n5. Using the same `orders` `Series`, create a `DataFrame` with columns for `order number`, `date`, and `shipped`.  `shipped` is a `boolean` value. \n Convert the dates to `datetime` which can convert mixed formats.  Note that the modifier `{n}` can be used to specify an exact number of matched characters.  Don't include lines which don't contain a date.  Save the `DataFrame` in a variable called `order_table` and print it.\n\n\n\n---\n\n### **Submit the Notebook for Your Assignment**  \n\n📌 **Follow these steps to submit your work:**  \n\n#### **1️⃣ Get a Sharing Link for Your Assignment**  \n- On the upper right of the Kaggle page, click on Save Version and save, accepting all defaults.  You can just do a quick save.\n- On the upper right, click on Share.  Choose Public, make sure that Allow Comments is on, and copy the public URL to your clipboard.\n\n#### **2️⃣ Submit Your Kaggle Link**  \n- Paste the URL into the **assignment submission form**.  \n\n---\n",
        "codeExample": "",
        "_id": "68796c78a4ac7ef2a17a618a"
      }
    ],
    "submissionInstructions": "Please submit on time",
    "checklist": [],
    "checkForUnderstanding": []
  },
  "subsections": [
    {
      "subsectionOrder": 1,
      "title": "Lesson 7",
      "content": "\n# **Lesson 07 — Data Cleaning and Validation with Pandas**\n\n## **Lesson Overview**\n**Learning objective:** Students will learn to clean and standardize real-world datasets using Pandas. They will handle missing data, outliers, duplicates, inconsistent formatting, and categorical variables, while also applying transformations and basic feature engineering techniques to prepare data for analysis or modeling.\n\n\n### **Topics:**\n1. **Handling Missing Data**: Using `dropna()` and `fillna()` to manage null values.\n2. **Data Transformation**: Changing data types and formatting dates for analysis.\n3. **Using Regular Expressions**: Using regex with `str.replace()`, `extract()`, and `contains()` in Pandas.\n4. **Removing Duplicates**: Identifying and removing repeated rows with `drop_duplicates()`.\n5. **Handling Outliers**: Replacing extreme values with statistical measures like the median.\n6. **Standardizing Data**: Lowercasing, trimming whitespace, and mapping values for consistency.\n7. **Validating Data Ranges**: Ensuring numeric values fall within expected bounds.\n8. **Handling Categorical Data**: Encoding categories with label encoding and one-hot encoding using `get_dummies()`.\n9. **Handling Inconsistent Data**: Using regex and string methods to normalize inconsistent entries.\n10. **Feature Engineering**: Binning continuous variables into categories with `pd.cut()`.\n\n---\n\n## **6.1 Handling Missing Data**\n\n### **Overview**\nMissing data can affect the accuracy and reliability of analysis. Common approaches include removing or replacing missing values.\n\n### **Key Methods:**\n- `dropna()`: Removes rows or columns with missing values.\n- `fillna()`: Replaces missing values with specified replacements like the mean, median, or a default value.\n\n### **Why Handle Missing Data?**\n- Ensures consistent dataset formatting.\n- Avoids errors during calculations.\n- Preserves data integrity.\n\n### **Code Example:**\n```python\nimport pandas as pd\n\n# Sample DataFrame\ndata = {'Name': ['Alice', None, 'Charlie', 'David'],\n        'Age': [24, None, 22, 35],\n        'Salary': [50000, 60000, None, None]}\ndf = pd.DataFrame(data)\n\n# Drop rows with missing data\ndf_dropped = df.dropna()\n\n# Replace missing values\ndf_filled = df.fillna({'Name': 'Unknown', 'Age': df['Age'].mean(), 'Salary': 0})\n\nprint(\"DataFrame with missing data handled:\")\nprint(df_filled)\n```\n\n### **Explanation:**\n- `dropna()` removes any row that contains a `None` (missing) value.\n- `fillna()` is used to replace missing values. In this case, the `Age` column's missing values are replaced with the mean, and the `Salary` column's missing values are filled with 0.\n\n---\n\n## **6.2 Data Transformation**\n\n### **Overview**\nData transformation converts data into consistent formats and types for easier analysis and calculations.\n\n### **Key Tasks:**\n- Converting data types.\n- Formatting date columns for temporal analysis.\n\n### **Why Transform Data?**\n- Ensures compatibility with numerical and time-based functions.\n- Reduces inconsistencies in datasets.\n\n### **Code Example:**\n```python\n# Sample DataFrame\ndata = {'Name': ['Alice', 'Bob', 'Charlie'],\n        'Age': ['24', '27', '22'],\n        'Join_Date': ['2023-01-15', '2022-12-20', '2023-03-01']}\ndf = pd.DataFrame(data)\n\n# Convert 'Age' to integer\ndf['Age'] = df['Age'].astype(int)\n\n# Convert 'Join_Date' to datetime\ndf['Join_Date'] = pd.to_datetime(df['Join_Date'])\n\nprint(\"Transformed DataFrame:\")\nprint(df)\n```\n\n### **Explanation:**\n- `astype(int)` converts the `Age` column, originally stored as strings, into integers.\n- `pd.to_datetime()` converts the `Join_Date` column into Python’s datetime objects for easier date manipulation and comparison.\n\n---\n\n## **6.3 Using Regular Expressions**\n\n### A brief introduction to regular expressions\n\n#### History\n- Invented as a theoretical framework in 1951 by Stephen Cole Kleene\n- First used in 1968 for the QED text editor (Ken Thompson)\n- Used in a variety of Unix tools in the 1970's (ed, lex, vi, grep, awk, emacs)\n- Now the most popular string pattern matching language, available in almost all programming languages\n- Most standardized on the perl version which is more powerful and less verbose than other standards\n\n#### Regular expression syntax and matching\n- Python provide regular expressions through the standard library [re module](https://docs.python.org/3/library/re.html)\n- The [regex HOWTO](https://docs.python.org/3/howto/regex.html#regex-howto) provides a tutorial on usage\n- For more practice with regexes [regex101](https://regex101.com/) is a useful site.\n- Raw strings (`r''`) are preferred for regexes in python to avoid excessive `'\\'` characters\n- All regular characters match themselves\n- Meta characters extend patterns for more complex matches\n  - Meta-characters can be included in patterns by escaping with `‘\\’`\n  - `<pattern>*` : match 0 or more copies of pattern\n  - `<pattern>+` : match 1 or more copies of pattern\n  - `<pattern>{n}` : match exactly `n` copies of the pattern\n  - `<pattern>?` : match 0 or 1 times\n  - `^<pattern>` : match only at the beginning\n  - `<pattern>$` : match only at the end\n  - Character sets: `[a-z0-9]` – match any of the characters in the set\n  - Excluding characters: `[^A-Z]` `^` means match characters not in the set (e.g. anything but capital letters)\n    - `^` has a different meaning in this context, specifying a character exclusion rather than the start of the string\n  - `\\d` : match digits (short for `[0-9]`), `\\D` not a digit\n  - `\\w` : match any alphanumeric (word - short for `[a-zA-Z0-9_]`), `\\W` not a word character\n  - `\\s` : match whitespace, `\\S` not a whitespace character\n  - `.` : match any character except newline. Use `\\.` to match a period.  I general `\\` escapes special characters.\n  - Use `|` to specify alternates: `[0-9]+|[a-z]+`:  e.g. match digits or lowercase letters\n    - Surrounding the alternative by `()` limits the scope of the alternatives e.g. `r'start ([0-9]+|[a-z]+) end'`.  This also introduces a match group which is not necessarily used.\n  - Group using `()` to capture substrings\n\n#### Examples\n- `^[a-zA-Z]\\w+`   # Must start at the beginning of the string with a letter followed by any word character\n- `[0-9]+|[a-z]+`  # Match either one or more digits or one or more lowercase letters (but not both)\n- `[a-z]|[0-9]+`   # Match one lowercase letter or one or more digits (but not both)\n- `^\\s*(\\w+)\\s*$`  # Capture a word which may have whitespace before or after\n\n#### Debugging regular expressions\n\nRegular expressions are greedy\n- Eeach part of the pattern matches as many characters as possible\n- Bugs often involve a part of the pattern matching too much (or all) of the rest of the string\n- e.g. putting ^.+ at the beginning of the pattern will match all the characters\n  - Leaving nothing for the rest of the pattern\n\n\n### Using regular expressions with the Python Standard Library\n\nIn the python standard library [re module](https://docs.python.org/3/library/re.html), regular expressions are precompiled into a [pattern object](https://docs.python.org/3/library/re.html#re-objects).  The pattern object can then be matched against entire strings using the [match() method](https://docs.python.org/3/library/re.html#re.Pattern.match) or the [search() method](https://docs.python.org/3/library/re.html#re.Pattern.search) can be used to scan for the pattern in a string.  These methods return `None` if there is no match.  If there is a successful match, a [match object](https://docs.python.org/3/library/re.html#re.Match) is returned.  The [group() method](https://docs.python.org/3/library/re.html#re.Match.group) can be used to retrieve the entire match or subgroups defined using parentheses.  For this class, we will be using regular expressions with Pandas as describe below, however it can be helpful to use the standard library regular expressions to develop and test a pattern.  It's also an important part of the Python ecosystem.  Here are some examples:\n\n```python\nimport re\n# compile the regular expression\n# This regex captures the username from a gmail address as a subgroup\n# It requires one or more word chacaters at the beginning, then any number of words, digits, periods and '-''s\n# The first `()` will be available a group(1) if there is a match\ngmail = re.compile(r'(\\w+[\\w\\d\\.\\-]*)@gmail.com')\nsearch_target = 'Boa-Dreamcode.public@gmail.com'\n# match the entire string\nmatch = gmail.match(search_target)\n# print the group and subgroup\nprint(match.group()) # => Boa-Dreamcode.public@gmail.com (same as group(0))\nprint(match.group(1)) # => Boa-Dreamcode.public\n# now serch within a longer string\nsearch_target = 'My email is Boa-Dreamcode.public@gmail.com, what is yours?'\nmatch = gmail.search(search_target)\nprint(match.group()) # => Boa-Dreamcode.public@gmail.com (same as group(0))\nprint(match.group(1)) # => Boa-Dreamcode.public\n```\n\n### Using regular expressions with Pandas\n  \n Pandas [Series.str](https://pandas.pydata.org/pandas-docs/stable/reference/series.html#string-handling) provides a variety of methods which access or manipulate the values of a Series as strings.  Many of these methods support regular expressions.  All of the examples below assume you have imported `pandas as pd`.  The [Series.filter](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.filter.html#pandas.Series.filter) method also supports regular expressions.  The methods available with `Series.str` are different from those provided by the builtin `str` type.\n\n #### Using `replace`\n\n Sometimes phone numbers or ID fields can contain non-numeric characters (such as dashes, parentheses, or letters) that you want to remove.\n\n ```python\n df = pd.DataFrame({\n    'phone_number': ['(123) 456-7890', '+1-555-123-4567', '555.456.7890']\n})\n# Remove all non-digit characters\ndf['phone_number_clean'] = df['phone_number'].str.replace(r'\\D', '', regex=True)\nprint(df)\n```\n\nRemoving html tags from scraped data.\n\n```python\ndf = pd.DataFrame({\n    'html_content': [\n        '<p>This is a paragraph.</p>',\n        '<div>Some <strong>bold</strong> text</div>'\n    ]\n})\n# <.*?> matches <, then any characters as few as possible (.*? is a non-greedy match), then >\ndf['text_only'] = df['html_content'].str.replace(r'<.*?>', '', regex=True)\nprint(df)\n```\n\n#### Using `extract`\n\nCapture the domain name from email addresses.  Note that the `extract` method only uses regular expressions, whereas they are optional and must be specified for `replace`.\n\n```python\ndf = pd.DataFrame({\n    'email': [\n        'john.doe@example.com',\n        'jane_smith@my-domain.org',\n        'user123@anotherdomain.net'\n    ]\n})\ndf['domain'] = df['email'].str.extract(r'@(\\w+[\\w\\.-]+)')\nprint(df)\n```\nMatch groups can be given names.  When named groups are used with the `dataFrame.extract` method the group names are used as column names in the resulting `DataFrame`.\n\n```python\nseries = pd.Series(['Tom-25-USA', 'Anna-30-UK', 'John-22-Canada'])\npattern = r'(?P<Name>\\w+)-(?P<Age>\\d+)-(?P<Country>\\w+)'\nresult = series.str.extract(pattern)\nprint(result)\n\n```\n\n#### Using `contains`\n\nGet all rows which contain valid emails.\n\n```python\ndf = pd.DataFrame({\n    'email': ['test@example.com', 'invalid-email', 'hello@mydomain.org']\n})\nvalid_emails = df[df['email'].str.contains(r'^\\w+[\\w\\.-]*@\\w+[\\w\\.-]+\\.\\w+$')]\nprint(valid_emails)\n```\n#### Combining filters using bitwise operators\nThe `contains` method returns a `Series` of boolean values also known as a filter.  Bitwise operators (&, |, ~) can be used to combine or invert filters.  Don't confuse these with the boolean operators (`and`, `or`, and `not`). Also, note that the raw strings used to define regular expressions can be entered on multiple lines.  Regexs can be long, and listing them on multiple lines improves readability.  Here a series of alternatives are listed one per line with a trailing `'|'`.  Note that they aren't separated by commas.\n\n```python\norders = [\n    \"Order 1: 2x Cheddar, 1x Gouda\",\n    \"Order 2: 3x Stilton, 2x Rye Crackers\",\n    \"Order 3: 2x Saltines\",\n    \"Order 4: 1x Camembert, 2x Jahrlsberg\",\n    \"Order 5: 2x Gouda, 2x Rye Crackers\",\n    \"Order 6: 1x Ritz, 1x Jahrlsberg\",\n    \"Order 7: 1x Parmesan, 1x Brie\",\n    \"Order 8: 3x Saltine Crackers\",\n    \"Order 9: 2x Rye Crackers\",\n    \"Order 10: 2x Mozzarella, 1x Cheddar\",\n    \"Order 11: 1X Water Crackers\"\n    \"Order 12: 3x Blue Cheese\",\n    \"Order 13: 1x Triscuits\",\n    \"Order 14: 1x Butter Crackers, 2x Multigrain Crackers\",\n    \"Order 15: 1x Feta\",\n    \"Order 16: 1x Havarti\",\n    \"Order 17: 2x Wheat Crackers\",\n    \"Order 18: 1x Ricotta\",\n    \"Order 19: 1x Garlic Herb Crackers\"\n]\norders = pd.Series(orders)\nfavored_cheeses = orders.str.contains(r'Cheddar|'\n                                       r'Stilton|'\n                                       r'Camembert|'\n                                       r'Jahrlsberg|'\n                                       r'Gouda', case=False, regex=True)\nfavored_crackers = orders.str.contains(r'Ritz|'\n                                       r'Triscuit|'\n                                       r'Rye Crackers|'\n                                       r'Multigrain Crackers|'\n                                       r'Water Crackers', case=False, regex=True)\nprint(orders[favored_cheeses | favored_crackers])\nprint(orders[favored_cheeses & favored_crackers])\nprint(orders[~favored_cheeses])\n```\n\n#### Using `filter`\n\nInstead of specifying columns one by one, you can select or drop columns whose names match a pattern using DataFrame.filter.\n\n```python\ndf = pd.DataFrame({\n    'col_2021': [1, 2, 3],\n    'col_2022': [4, 5, 6],\n    'col_other': [7, 8, 9]\n})\n# Select columns that end with digits\ndf_year = df.filter(regex=r'\\d+$')  \nprint(df_year)\n```\n\n## **6.4 Removing Duplicates**\n\n### **Overview**\nDuplicates can distort analysis by inflating metrics or introducing bias. Identifying and removing them ensures data quality.\n\n### **Key Method:**\n- `duplicated()`: Identifies duplicates.\n- `drop_duplicates()`: Removes duplicate rows.\n\n### **Code Example:**\n```python\n# Sample DataFrame\ndata = {'Name': ['Alice', 'Bob', 'Alice', 'David'],\n        'Age': [24, 27, 24, 35],\n        'Salary': [50000, 60000, 50000, 80000]}\ndf = pd.DataFrame(data)\n\n# Remove duplicates\ndf_no_duplicates = df.drop_duplicates()\n\nprint(\"DataFrame with duplicates removed:\")\nprint(df_no_duplicates)\n```\n\n### **Explanation:**\n- `drop_duplicates()` removes rows where the entire record is a duplicate of another.\n- `drop_duplicates(subset='Name')` removes rows where the `Name` column is duplicated, keeping only the first occurrence of each name.\n\n---\n\n## **6.5 Handling Outliers**\n\n### **Overview**\nOutliers are extreme values that deviate significantly from other observations and can bias statistical calculations.\n\n### **Common Approach:**\n- Replace outliers with statistical measures like the median.\n\n### **Code Example:**\n```python\n# Replace outliers in 'Age' (e.g., Age > 100 or Age < 0)\ndf['Age'] = df['Age'].apply(lambda x: df['Age'].median() if x > 100 or x < 0 else x)\n\nprint(\"DataFrame after handling outliers:\")\nprint(df)\n```\n\n### **Explanation:**\n- Outliers in the `Age` column that are greater than 100 or less than 0 are replaced by the median value of the `Age` column.\n\n---\n\n## **6.6 Standardizing Data**\n\n### **Overview**\nStandardizing text data ensures consistency, making it easier to group, filter, and analyze.\n\n### **Key Techniques:**\n- Convert text to lowercase and strip whitespace.\n- Standardize inconsistent entries.\n\n### **Code Example:**\n```python\n# Standardize 'Name' column\ndf['Name'] = df['Name'].str.lower().str.strip()\n\n# Standardize 'City' column with mapping\ndf['City'] = df['City'].replace({'ny': 'New York', 'la': 'Los Angeles'})\n\nprint(\"Standardized DataFrame:\")\nprint(df)\n```\n\n### **Explanation:**\n- The `Name` column is standardized by converting all entries to lowercase and stripping any extra whitespace.\n- The `City` column is standardized by replacing `ny` with `New York` and `la` with `Los Angeles`.\n\n---\n\n## **6.7 Validating Data Ranges**\n\n### **Overview**\nValidating data ranges ensures all values fall within acceptable limits, avoiding invalid or erroneous data.\n\n### **Example Task:**\n- Ensure ages are between 18 and 65. Replace invalid values with `NaN` and fill them with the median.\n\n### **Code Example:**\n```python\n# Replace invalid ages with NaN\ndf['Age'] = df['Age'].apply(lambda x: x if 18 <= x <= 65 else None)\n\n# Fill missing values with median\ndf['Age'] = df['Age'].fillna(df['Age'].median())\n\nprint(\"DataFrame after validating age ranges:\")\nprint(df)\n```\n\n### **Explanation:**\n- Any age outside the range of 18 to 65 is replaced with `None` (NaN).\n- Missing values in the `Age` column are filled with the median value of the column.\n\n---\n\n## **6.8 Handling Categorical Data**\n\n### **Overview**\nHandling categorical data involves encoding non-numeric values, which is especially useful for machine learning models that require numerical input.\n\n### **Key Techniques:**\n- **Label Encoding**: Converting each category into a number.\n- **One-Hot Encoding**: Creating binary columns for each category.\n\n### **Why Handle Categorical Data?**\n- Many machine learning algorithms require numerical data, so we need some way to convert categories into numbers.\n- Proper encoding helps preserve the categorical structure in the data. There are different ways to represent categorical data numerically: with [one hot encoding](https://www.datacamp.com/tutorial/one-hot-encoding-python-tutorial) each category is represented in a binary fashion as present or absent: this is a very popular technique in machine learning. \n- In pandas, one-hot-encoding is implemented with the `get_dummies()` function. \n\n### **Code Example:**\n```python\n# Sample DataFrame with categorical data\ndata = {'Color': ['Red', 'Blue', 'Green', 'Blue', 'Red']}\ndf = pd.DataFrame(data)\n\n# Label encoding: Convert categories to numbers\ndf['Color_Label'] = df['Color'].map({'Red': 1, 'Blue': 2, 'Green': 3})\n\n# One-Hot Encoding: Create binary columns for each category\ndf_encoded = pd.get_dummies(df['Color'], prefix='Color')\n\nprint(\"DataFrame with Categorical Data Handled:\")\nprint(df_encoded)\n```\n\n### **Explanation:**\n- **Label Encoding** maps the `Color` column's categories to integer values.\n- - **One-Hot Encoding** use the `get_dummies()` function to create binary columns for each unique value in the `Color` column. \n---\n\n## **6.9 Handling Inconsistent Data**\n\n### **Overview**\nInconsistent data can result from typos, different formats, or various naming conventions. Handling inconsistencies ensures uniformity in the dataset.\n\n### **Key Techniques:**\n- **Fuzzy Matching**: Identifying and standardizing similar but non-exact values.\n- **Regex (Regular Expressions)**: Using patterns to extract or replace inconsistent data.\n\n### **Why Handle Inconsistent Data?**\n- Improves the quality of data for analysis.\n- Helps identify patterns across otherwise unmatchable data points.\n\n### **Code Example:**\n```python\nimport re\n\n# Sample DataFrame with inconsistent data\ndata = {'City': ['New York', 'new york', 'San Francisco', 'San fran']}\ndf = pd.DataFrame(data)\n\n# Standardize text data (convert to lowercase and strip spaces)\ndf['City'] = df['City'].str.lower().str.strip()\n\n# Use Regex to replace shorthand names\ndf['City'] = df['City'].replace({'san fran': 'san francisco'})\n\nprint(\"DataFrame with Inconsistent Data Handled:\")\nprint(df)\n```\n\n### **Explanation:**\n- **String standardization**: Converts all entries in the `City` column to lowercase and removes extra spaces.\n- **Regex**: Matches and replaces shorthand for cities with their full names.\n\n---\n\n## **6.10 Feature Engineering**\n\n### **Overview**\nFeature engineering involves creating new features from existing ones in raw data. \nThis is typically used as a first step before the data is fed to machine\nlearning algorithms. We will go over one way to extract features from data, *binning*\ninto discrete categories, but [feature engineering is a large subfield of its own](https://www.ibm.com/think/topics/feature-engineering).\n\n### **Key Techniques:**\n- **Binning**: Categorizing continuous data into discrete bins.\n- **Principal component analysis (PCA)**: for automated extraction of important combinations of features from high dimensional datasets (this is a fairly advanced topic, but if you are interested in an intuitive overview of PCA see [this video](https://www.youtube.com/watch?v=ZgyY3JuGQY8)). \n\n### **Why Feature Engineering?**\n- New features can reveal hidden patterns and relationships.\n- Reduce noise in the raw data.\n- Improves model performance in machine learning.\n\n### **Code Example:**\n```python\n# Sample DataFrame with numerical data\ndata = {'Age': [24, 35, 30, 45, 60]}\ndf = pd.DataFrame(data)\n\n# Binning Age into age groups\nbins = [0, 30, 60, 100]\nlabels = ['Young', 'Middle-Aged', 'Old']\ndf['Age_Group'] = pd.cut(df['Age'], bins=bins, labels=labels)\n\nprint(\"DataFrame after Feature Engineering:\")\nprint(df)\n```\n\n### **Explanation:**\n- **Binning**: Converts `Age` into categories like 'Young', 'Middle-Aged', and 'Old' based on defined intervals.\n\n---\n\n## **Summary**\n\nIn this lesson, you learned how to:\n1. Handle missing data with `dropna()` and `fillna()`.\n2. Transform data types and formats.\n3. Remove duplicate records.\n4. Identify and manage outliers.\n5. Standardize text data for consistency.\n6. Validate data ranges to ensure accuracy.\n7. Handle categorical data with encoding methods.\n8. Address inconsistent data with fuzzy matching and regex.\n9. Apply feature engineering for better insights and analysis.\n\nThese techniques are essential for maintaining clean, reliable datasets, ready for analysis. Explore the [Pandas Documentation](https://pandas.pydata.org/docs/) to deepen your understanding.\n",
      "videoUrl": "",
      "codeExamples": [],
      "externalLinks": [],
      "quizzes": [],
      "_id": "68796c78a4ac7ef2a17a6189"
    }
  ]
}