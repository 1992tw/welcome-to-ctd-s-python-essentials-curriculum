{
  "id": "68796c79a4ac7ef2a17a6194",
  "lessonNumber": 10,
  "title": "Lesson 10 ‚Äî Introduction to Web Scraping",
  "status": "pending",
  "assignment": {
    "title": "Assignment for Lesson 10",
    "objective": "No objective specified",
    "expectedCapabilities": [],
    "instructions": [],
    "tasks": [
      {
        "taskNumber": 1,
        "title": "Task 1",
        "description": "# **Introduction to Web Scraping**\n\n### **Objective**\n\nYou learn to harvest data from a live web site, scraping values from the HTML page and storing them in a CSV file.\n\n## **Assignment Instructions**\n\nYou create the code for this assignment in the assignment10 folder within your python_homework folder.  Be sure to create an `assignment10` git branch before you start.  As usual, mark the code that completes each task with a comment line.\n\n---\n\n## **Overview**\nThis lesson introduces web scraping concepts and practices, including:\n1. Understanding HTML and the DOM structure in a live web page.\n2. Extracting content using Selenium.\n3. Storing data in CSV and JSON files.\n4. Ethical considerations in web scraping.\n\n\n---\n\n## **Tasks**\n\n### **Task 1: Review robots.txt to Ensure Policy Compliance**\n\nIn this assignment, you collect data from the Durham County Library site.  As always, when you do web scraping, you need to review the policy described in robots.txt, to ensure that you comply.\n\n1. Open [https://durhamcountylibrary.org/robots.txt]\n\n2. Verify that the following steps are not in breach of policy.\n\n### **Task 2: Understanding HTML and the DOM for the Durham Library Site**\n\nYou are going to find where in the HTML page the data you want is located.\n\n1. Open [https://durhamcounty.bibliocommons.com/v2/search?query=learning%20spanish&searchType=smart]\n\n2. Open your browser developer tools to show the HTML elements.  For Chrome, this is `shift-ctrl-J`.\n\n3. Find the HTML element for a single entry in the search results list.  This is a little tricky.  When you select an element in the Chrome developer tools, that part of the web page is highlighted.  But, that typically only gets you to the main `div`.  There is a little arrow next to that div in the developer tools element window, and you click on that to open it up to show the child elements.  Then, you select the element that corresponds to the search results area.  Continue in this way, opening up divs or other containers and selecting the one that highlights the area you want.  Eventually, you'll get to the first search result.  Hint: You are looking for an `li` element, because this is an element in an unordered list.  Note the values for the class attribute of this entry.  You'll want to save these in some temporary file, as your program will need them.\n\n3. Within that element, find the element that stores the title.  Note the tag type and the class value.  Your program will need this value too, so save it too.\n\n4. Within the search results `li` element, find the element that stores the author.  Hint: This is a link.  Note the class value and save it.  Some books do have multiple authors, so you'll have to handle that case.\n\n5. Within the search results `li` element, find the element that stores the format of the book and the year it was published.  Note the class value and save it.  Now in this case, the class might not be enough to find the part of the `li` element you want.  So look at the `div` element that contains the format and year.  You want to note that class value too.\n\n### **Task 3: Write a Program to Extract this Data**\n\n1. `get_books.py`. The program should import from selenium and webdriver_manager, as shown in your lesson.  You also need pandas and json.\n\n2. Add code to load the web page given in task 2.\n\n3. Find all the `li` elements in that page for the search list results.  You use the class values you stored in task 2 step 3.  Also use the tag name when you do the find, to make sure you get the right elements.\n\n5. Within your program, create an empty list called `results`.  You are going to add `dict` values to this list, one for each search result.\n\n6. Main loop: You iterate through the list of `li` entries.  For each, you find the entry that contains title of the book, and get the text for that entry.  Then you find the entries that contain the authors of the book, and get the text for each.  If you find more than one author, you want to join the author names with a semicolon `;` between each.  Then you find the `div` that contains the format and the year, and then you find the `span` entry within it that contains this information.  You get that text too.  You now have three pieces of text.  Create a `dict` that stores these values, with the keys being `Title`, `Author`, and `Format-Year`.  Then append that `dict` to your `results` list.\n\n7. Create a DataFrame from this list of dicts.  Print the DataFrame.\n\n**Hint** You can put little print statements in at each step, to see if that part works.  For example, when you find all the `li` entries, you could print out the length of the list.  Then, you could just implement the part of the main loop that finds the Title, and just print out that title.  In this way, you can get the program done incrementally.\n\n**For Further Thought**  You are getting the search results from the first page of the search results list.  How would you get all the search results from all the pages?  How can you make the program do this regardless of how many pages you might have?  **Optional:** Change your program to page through the search results so as to get all of the results.  However! You need to make sure your program pauses between pages.  Fast screen scraping, where many requests are sent in short order, is an abuse of the privilege.\n\n### **Task 4: Write out the Data**\nModify your program to do the following:\n\n1. Write the DataFrame to a file called `get_books.csv`, within the assignment10 folder.  Examine the file to see if it looks right.\n\n2. Write the `results` list out to a file called `get_books.json`, also within the assignment10 folder.  You should write it out in JSON format.  Examine the file to see if it looks right.\n\n### **Task 5: Ethical Web Scraping**\n\n**Goal**:  \nUnderstand the importance of ethical web scraping and `robots.txt` files.\n\n1. Access the `robots.txt` file for Wikipedia: [Wikipedia Robots.txt](https://en.wikipedia.org/robots.txt).\n2. Analyze the file and answer the following questions.  Put your answers in a file called `ethical_scraping.txt` in your python_homework/assignment10 directory\n   - Which sections of the website are restricted for crawling?\n   - Are there specific rules for certain user agents?\n3. Reflect on why websites use `robots.txt` and write 2‚Äì3 sentences explaining its purpose and how it promotes ethical scraping.  Put these in `ethical_scraping.txt` in your python_homework directory.\n\n\n---\n\n### **Task 6: Scraping Structured Data**\n\n**Goal**:  \nExtract a web page section and store the information.\n\n1. Use your browser developer tools to view this page: [https://owasp.org/www-project-top-ten/].  You are going to extract the top 10 security risks reported there.  Figure out how you will find them.\n\n2. Within your python_homework/assignment10 directory, write a script called `owasp_top_10.py`.  Use selenium to read this page.\n\n3. Find each of the top 10 vulnerabilities.  Hint: You will need XPath.  For each of the top 10 vulnerabilites, keep the vulnerability title and the href link in a dict.  Accumulate these dict objects in a list.\n\n4. Print out the list to make sure you have the right data.  Then, add code to the program to write it to a file called `owasp_top_10.csv`.  Verify that this file appears correct.\n\n5. Create a file, `challenges.txt`, also within your lesson9 directory.  In this file, describe any challenges you faced in completing this assignment and how you resolved them.\n\n---\n\n\n### Submit Your Assignment on GitHub**  \n\nüìå **Follow these steps to submit your work:**  \n\n#### **1Ô∏è‚É£ Add, Commit, and Push Your Changes**  \n- Within your python_homework folder, do a git add and a git commit for the files you have created, so that they are added to the `assignment10` branch.\n- Push that branch to GitHub. \n\n#### **2Ô∏è‚É£ Create a Pull Request**  \n- Log on to your GitHub account.\n- Open your `python_homework` repository.\n- Select your `assignment10` branch.  It should be one or several commits ahead of your main branch.\n- Create a pull request.\n\n#### **3Ô∏è‚É£ Submit Your GitHub Link**  \n- Your browser now has the link to your pull request.  Copy that link. \n- Paste the URL into the **assignment submission form**. \n\n---\n\n## **Resources**\n\n- [Selenium Documentation](https://www.selenium.dev/documentation/webdriver/)\n- [MDN Web Docs: Understanding the DOM](https://developer.mozilla.org/en-US/docs/Web/API/Document_Object_Model)\n- [OWASP robots.txt](https://owasp.org/robots.txt)\n\n---  \n",
        "codeExample": "",
        "_id": "68796c79a4ac7ef2a17a6196"
      }
    ],
    "submissionInstructions": "Please submit on time",
    "checklist": [],
    "checkForUnderstanding": []
  },
  "subsections": [
    {
      "subsectionOrder": 1,
      "title": "Lesson 10",
      "content": "\n# **Lesson 10 ‚Äî Introduction to Web Scraping**\n\n## **Lesson Overview**\n**Learning objective:** Students will gain a comprehensive understanding of web scraping, focusing on the fundamentals such as HTML structure, DOM representation, and using Python libraries like `Selenium` and `WebDriver Manager` to scrape and extract data from web pages. Additionally, students will explore the ethical aspects of web scraping, including adhering to guidelines provided by `robots.txt` and managing server requests responsibly.\n\n### **Topics:**\n1. Basics of HTML and DOM\n2. Web Scraping Tool: `Selenium` and `WebDriver Manager`\n3. Ethical Web Scraping: Understanding `robots.txt` and ethical considerations\n4. Scraping Structured Data: Extracting specific data points\n5. Managing Requests: Delays, retries, and handling errors\n6. Saving Scraped Data to Files\n7. Frailty in Scraping Programs\n\n**Why Do Web Scraping?**\n\nThere is a lot of information on the Internet, as you know, but it is in web pages that are provided so that humans can read them.  If you want to automate the process of searching or archiving some of this information, you need to extract it into a structured format and store it separately, so that you can then do SQL searches or the like on what you save.\n\nHere are some examples:\n\n- Finding the prices of airline flights to a given destination for multiple airlines.\n- Finding all scholarly articles on a particular topic you want to research and saving this information for reference.\n- Finding the schedule of public meetings for local government bodies.\n\n---\n\n## **9.1 Basics of HTML and DOM**\n\n### **Overview**\nHTML (Hypertext Markup Language) is the backbone of web pages, providing structure and content. The Document Object Model (DOM) is a tree structure that represents the page content. Understanding the structure of web pages and the DOM is essential for locating and extracting data during web scraping.  When you pull up a web page in your browser, a request is sent to a web server and an HTML document is returned.  Your browser then parses that document and presents the page.  When you do web scraping, you'll also parse that page, not for presentation, but to capture information from that page so that it can be stored in a structured way.  You need to understand HTML and the DOM structure to do screen scraping.  Many of you may already be familiar with HTML -- if so, skip ahead.\n\n### Elements of an HTML Document\n\nEach element of the DOM has the following:\n- A tag, the name for the type of element.\n- Attributes.  These are name-value pairs, and vary with the element type.  For example, a `class` attribute might describe the display style for the element.  For a link element, the `href` attribute describes web address to use for the link.  There are many others.\n- Content.  This may be text, or it may be other elements.  As the DOM is a tree, some elements contain others.\n\n### **Key HTML Elements:**\n- **`<title>`**: The title of the page.\n- **`<p>`**: Paragraphs of text.\n- **`<h1>, <h2>, <h3>`**: Headers of varying levels.\n- **`<a>`**: Links with `href` attributes pointing to URLs.\n- **`<img>`**: Images with `src` attributes indicating the image source.\n\nThere are many others.\n\n### For Further Reference\n\nA good online refernce to HTML is available at [W3Schools](https://www.w3schools.com/html/).\n\n### **Hands-On Activity:**\n\n1. Open the Wikipedia page for \"Web Scraping\" ([Wikipedia - Web Scraping](https://en.wikipedia.org/wiki/Web_scraping)).\n2. Open the developer tools for your browser.  For Chrome this is `Shift-Ctrl-J`.  Inspect the elements of the page.  In the Chrome developer tools, they are in the `Elements` tab. \n3. Traverse this collection of elements to:\n   - Identify the title of the page.\n   - Locate the first paragraph and examine its structure.\n   - Explore headers (`<h1>, <h2>, <h3>`) and links (`<a>`).\n\nWhen you are creating a web scraping program, this is often the first step.  Look at the page you want to scrape in the browser developer tools and find where the information you want resides within the DOM, so that you can write code to extract it.\n\n### **How the DOM Is Structured:**\nThe DOM is a hierarchical tree structure. Here's an example:\n```html\n<html>\n  <head>\n    <title>Web Scraping</title>\n  </head>\n  <body>\n    <h1>Introduction</h1>\n    <p>This is an example paragraph.</p>\n    <a href=\"https://example.com\">Example Link</a>\n  </body>\n</html>\n```\n\n---\n\n## **9.2 Web Scraping Tools: Selenium and WebDriver Manager**\n\n### **Overview and Setup**\n\nTo scrape data from a web page, two primary libraries are commonly used in Python:\n- **`Selenium`**: A library that executes the same actions as a web browser, and makes the resulting page available for subsequent operations.\n- **`WebDriver Manager`**: Selenium requires a driver, which performs the actual web operations, just as a browser would.  This library makes the appropriate driver available to your program in an automated way.\n\nYou need to install these into your python_homework directory.  Within your VSCode terminal session for `python_homework` do:\n\n```bash\npip install selenium\npip install webdriver-manager\n```\n\n### **Steps to Web Scraping:**\n1. Initialize Selenium and the appropriate driver.  We'll use the Chromium driver, which is the same one as is in Chrome and other browsers.\n2. Retrieve a web page.\n3. Extract the desired elements using CSS selectors.\n\nThis is the initialization you'll need to do:\n\n```python\nfrom selenium import webdriver\nfrom selenium.webdriver.chrome.service import Service as ChromeService\nfrom webdriver_manager.chrome import ChromeDriverManager\nfrom selenium.webdriver.common.by import By\n\ndriver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))\n\n```\n\nOnce you have a driver, you can fetch a web page, as follows:\n\n```python\ndriver.get(\"https://en.wikipedia.org/wiki/Web_scraping\")\n```\n\n### **Explanation: Why do we Need So Much Machinery?**\n\nThis may seem a pretty heavy collection of stuff just to scrape a web page.  However, most web pages these days are dynamic.  They contain JavaScript, and after the page loads, the scripts run to populate the page.  So, we need an engine that will not only get the page, but also run the JavaScript, and the JavaScript may itself make Ajax or Fetch calls to get more data.  A lot happens to create the appearance of the page you see.\n\nTry the code above in your Python interative shell.  You'll see something a little odd.  A window will flash up with the actual page we are trying to scrape.  This can be interesting or annoying, depending on which side of the bed you got up on.  You can configure your driver to run in 'headless' mode, without a window, when you create the driver:\n\n```python\noptions = webdriver.ChromeOptions()\noptions.add_argument('--headless')  # Enable headless mode\noptions.add_argument('--disable-gpu')  # Optional, recommended for Windows\noptions.add_argument('--window-size=1920x1080')  # Optional, set window size\n\ndriver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()),options=options)\n\n```\n\n### **Finding Information**\n\nA web page is comprised of HTML elements.  You can find elements within the page, and the most flexible way to find them is by CSS selector.  If you have done web page styling, you have used CSS selectors to specify which elements should have a particular style: font, color, background color, border, etc.  Selectors have various criteria.  Here are some examples:\n\n```\nSelect by element name:\n\n'p' : select all paragraphs\n'h2' : select all h2 headings\n\nSelect by element class:\n\n'.this-style-class' : All elements that have a class of this-style-class.  The . means select by class\n\nSelect by attribute:\n\n'[src]' : All elements that have a value for the src attribute, such as img elements.\n'[data-event=\"meeting\"]' : All elements that have a value for the data-event attribute equal to \"meeting\".\n\nThere are others.  There are also combined selectors:\n\n'div.main-container' : All div elements that have a class of main-container.\n```\n\nCSS selectors can be as complicated as need be, but typically you do not want to do complicated selection when web scraping, as this will make your scraping program frail.  If you want a full tutorial on CSS selectors, go here: [https://www.w3schools.com/css/css_selectors.asp]\n\nHere are some examples.  Try them out in your Python interative shell (assuming you have already loaded the web page into your driver as shown above):\n\n```python\nfrom selenium.webdriver.common.by import By\n\ntitle = driver.title # Find the title.  Parts of the header are accessed directly, not via find_element(), which only works on the body\nprint(title)\n\nbody = driver.find_element(By.CSS_SELECTOR,'body') # Find the first body element, typically only one\nif body:\n    links = body.find_elements(By.CSS_SELECTOR,'a') # Find all the links in the body.\n    if len(links) > 0:\n        print(\"href: \", links[0].get_attribute('href'))  # getting the value of an attribute\n\n\n\nmain_div = body.find_element(By.CSS_SELECTOR,'div[id=\"mw-content-text\"]')\nif main_div:\n    bolds = main_div.find_elements(By.CSS_SELECTOR,'b')\n    if len(bolds) > 0:\n        print(\"bolds: \",bolds[0].text)\n\n# Extract all images with their src attributes\nimages = [(img.get_attribute('src')) for img in body.find_elements(By.CSS_SELECTOR,'img[src]')] # all img elements with a src attribute\nprint(\"Image Sources:\", images)\n# hmm, this example uses a list comprehension.  We haven't talked about those.  This is the same as:\nimage_entries = driver.find_elements(By.CSS_SELECTOR,'img[src]')\nimages = []\nfor img in image_entries:\n    images.append(img.get_attribute('src'))\n\nprint(\"Image Sources:\", images)\n# You can see that list comprehensions are a useful shortcut in Python!\n```\n\nOnce you get an element, you can do the following (this is not an exhaustive list):\n\n- Get the text value, if any.\n- Get the value of any attributes.\n- Search the descendants.\n\nYou can then close the browser connection with:\n\n```python\ndriver.quit()\n```\n\nThe `driver.get()` method can raise an exception.  It's a good idea to do it in a try block:\n\n```python\ntry:\n    driver.get(\"https://nonsense.never.com\")\nexcept Exception as e:\n    print(\"couldn't get the web page\")\n    print(f\"Exception: {type(e).__name__} {e}\")\nfinally:\n    driver.quit()\n```\n\n**Note:** Selenium is not used only for web scraping.  It can automate interaction with a browser page. One common use is to do end-to-end testing.  You can type into the fields of forms, you can click on submit buttons, you can refresh the page, you can hit the back button, and so on.  We won't do any of this for this lesson.\n\n---\n\n## **9.3 Ethical Web Scraping**\n\n### **Overview**\nWhile web scraping can be a powerful tool, it is important to follow ethical guidelines and respect website owners‚Äô wishes to avoid excessive server load and legal issues.\n\n### **Key Concepts:**\n- **What is `robots.txt`?**\n  - A file that specifies which sections of a website can or cannot be accessed by web crawlers.\n- **Why is `robots.txt` important?**\n  - It helps website owners control the traffic to their site and avoid server overload.\n  - Ethical scraping involves reviewing and adhering to the rules set in `robots.txt`.\n\n### **Activity: Explore `robots.txt` for Wikipedia**\n1. Access the `robots.txt` file for Wikipedia: [Wikipedia Robots.txt](https://en.wikipedia.org/robots.txt).\n2. Identify restricted sections of the site.\n3. Discuss why these sections are restricted.\n\n### **Example Code: Accessing `robots.txt`**\n```python\nfrom selenium import webdriver\nfrom selenium.webdriver.chrome.service import Service as ChromeService\nfrom webdriver_manager.chrome import ChromeDriverManager\nfrom selenium.webdriver.common.by import By\n\ndriver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))\nrobots_url = \"https://en.wikipedia.org/robots.txt\"\ndriver.get(robots_url)\nprint(driver.page_source)\ndriver.quit()\n\n```\n\n---\n\n## **9.4 XPath to Traverse the DOM**\n\n### **Overview**\nAfter scraping basic data, you can refine your scraping strategy to focus on specific sections of a webpage.  Sometimes, though, the find by CSS selector approach does not suffice.  Have a look at the [Wikipedia Web Scraping page.](https://en.wikipedia.org/wiki/Web_scraping) Somewhere near the bottom, you see the `See also` section.  Suppose you want to scrape each of the links in that section.  You first open up the developer tools in your browser to see how you find the section.  Open up the elements tab and do a ctrl-f to find \"See also\".  The second hit gets you pretty close.  This is an `h2` element.  If you scroll down a little bit, you see a `div` further down in the page, and that div has the list of links you want.  But (sigh) there are no good identifiers like class or id or attribute to find this div.  So you go back to that h2 with \"See also\".  That h2 has the id of \"See_also\".  This helps some.  What you want to do is go up to the parent div, the one that contains that h2, call it parent_div.  Then you get the div that is the next div sibling of parent_div.  That's the one you want.\n\nIt is clear that finding by CSS selector won't work in this case.  You have to use another technique, called XPath.  There is an XPath tutorial [here.](https://www.w3schools.com/xml/xpath_intro.asp) We won't need most of XPath, but we do need to know the XPath axes, specifically parent and sibling.  Here's the code:\n\n```python\nfrom selenium import webdriver\nfrom selenium.webdriver.chrome.service import Service as ChromeService\nfrom webdriver_manager.chrome import ChromeDriverManager\nfrom selenium.webdriver.common.by import By\n\ndriver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))\ndriver.get(\"https://en.wikipedia.org/wiki/Web_scraping\")  # this much you've seen before\n\nsee_also_h2 = driver.find_element(By.CSS_SELECTOR,'[id=\"See_also\"]') # our starting point\nlinks = []\nif (see_also_h2):\n    parent_div = see_also_h2.find_element(By.XPATH, '..') # up to the parent div\n    if parent_div:\n        see_also_div = parent_div.find_element(By.XPATH,'following-sibling::div' ) # over to the div with all the links\n        link_elements = see_also_div.find_elements(By.CSS_SELECTOR, 'a')\n        for link in link_elements:\n            print(f\"{link.text}: {link.get_attribute('href')}\")\n            name = link.text.strip()\n            url = link.get_attribute(\"href\")\n            if name and url:\n                links.append({\"name\": name, \"url\": url})\n\n\ndriver.quit()\n```\n\n### **Testing on Another Page:**\n\nTest the script on different Wikipedia pages to ensure consistency and robustness.\n\n---\n\n## **9.5 Managing Requests and Handling Errors**\n\n### **Overview**\nWhen scraping large numbers of web pages, managing requests responsibly is essential. Techniques include delaying requests and handling connection errors.  \n\n- The `driver.get()` operation may raise an exception. Sometimes this is a timeout.  Or perhaps the network might be down.  It may not help to retry the request in this case.\n- The `driver.get()` operation may return an HTTP response code like 404 or 500.  Selenium does *not* raise an exception in this case.  Unfortunately, there is no easy way to access the HTTP response code in Selenium.  You can check if driver.title is the string you expect.\n- You can do a subsequent driver.get() to scrape another page, but if you do, you want to pause between pages.  It is an abuse to drive excessive load by many rapid requests.\n    ```python\n    from time import sleep\n    try:\n        driver.get('https://acme.com/index.html')\n        ... # extract data\n        sleep(2) # wait 2 seconds\n        driver.get('https://acme.com/another.html')\n    except Exception as e:\n        print(f\"An exception occurred: {type(e).__name__} {e}\")\n    finally:\n        driver.quit()\n\n    ```\n---\n\n## **9.6 Saving Scraped Data**\n\n### **Overview**\nOnce you've scraped valuable data, you may want to save it in a structured format, such as CSV or JSON.\n\n### **Saving Data to CSV:**\n```python\nimport csv\n\n# Save extracted data to a CSV file\nwith open('scraped_data.csv', 'w', newline='') as file:\n    writer = csv.writer(file)\n    writer.writerow([\"Name\", \"Link\"])\n    for link in links:\n        writer.writerow([link[\"name\"], link[\"url\"]])\n```\n\n### **Saving Data to JSON:**\n```python\nimport json\n\n# Save data to a JSON file\ndata = {\"links\": links}\nwith open('scraped_data.json', 'w') as json_file:\n    json.dump(data, json_file, indent=4)\n```\n\n### **Saving Data to an SQL Database**\n\nSuppose you are scraping from multiple sites.  You may want to record different information from each site, and keep track of which site provided which information.  \n\nFor example, you may scrape airline sites for destinations, prices, and departure times.  You could create three tables, one for airlines, which would record the airline name and URL, and then three others for destinations, prices, and departure_times.  There would be a many-to-many relationship between airlines and destinations, which might have where_we_fly as a join table.  There would be a one-to-many relationship between where_we_fly entries and departure_times, and a one-to-many relationship between departure_times and prices (for economy, business, and first class). This is a little more complicated, to be sure -- but think about your data model when you get ready to store web scraping data!\n\n## **9.7 Frailty in Web Scraping**\n\nWeb scraping programs can be frail.  The authors of the websites that you are accessing to scrape data can change the format of the page.  This can break the scraping logic, as the DOM may no longer be organized as you expect, or because the entries have different classes or ids.  Sometimes you can make your scraping more robust by finding known strings within the formatted page, and traversing from that point, but sometimes you can only log the error so as to trigger work on a fix.\n\n---\n\n## **Summary**\n\nIn this lesson, you learned:\n1. How to understand the structure of web pages using HTML and the DOM.\n2. How to use Python libraries `Selenium` and `WebDriver Manager` for web scraping.\n3. Ethical considerations of web scraping, including respecting `robots.txt`.\n4. Techniques for extracting specific data, handling errors, and managing requests.\n5. How to save scraped data to CSV and JSON formats for future use.\n\n### **Important Notes:**\n- Always respect the website‚Äôs `robots.txt` file and terms of service.\n- Avoid overloading a server with excessive scraping requests.\n- Use appropriate delay and retry mechanisms to manage web traffic.\n\nFor further learning, explore the [Selenium Documentation](https://www.selenium.dev/documentation/webdriver/).\n",
      "videoUrl": "",
      "codeExamples": [],
      "externalLinks": [],
      "quizzes": [],
      "_id": "68796c79a4ac7ef2a17a6195"
    }
  ]
}