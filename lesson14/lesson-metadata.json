{
  "id": "68796c79a4ac7ef2a17a61a4",
  "lessonNumber": 14,
  "title": "Lesson 14 — Web Scraping and Dashboard Project",
  "status": "pending",
  "assignment": {
    "title": "Assignment for Lesson 14",
    "objective": "No objective specified",
    "expectedCapabilities": [],
    "instructions": [],
    "tasks": [
      {
        "taskNumber": 1,
        "title": "Task 1",
        "description": "# Lesson 14 — Web Scraping and Dashboard Project\n\n## Lesson Overview\nIn this lesson, students will create **four distinct programs** that retrieve baseball history data from the **Major League Baseball History** website and display the results in an interactive dashboard. The project will involve **Selenium** for web scraping, data cleaning, transforming the data into a structured format, storing it in a SQLite database, querying it via command line, and presenting it using **Streamlit** or **Dash** with interactive visualizations.\n\n**Learning Objective:**  \nBy completing this project, students will:\n- Use **Selenium** to scrape data from a website.\n- Clean and transform the raw data into a structured format.\n- Store the data in a **SQLite** database, with each CSV file as a separate table.\n- Query the database using **joins** via command line.\n- Build an interactive dashboard using **Streamlit** or **Dash** to display the insights.\n\n## Projects\n\n### 1. **Web Scraping Program**  \n- **Goal**: Scrape data from [Major League Baseball History](https://www.baseball-almanac.com/yearmenu.shtml).\n- **Steps**:\n  - Use **Selenium** to retrieve the data.\n  - Extract relevant details (year, event names, statistics).\n  - Save the raw data into **CSV** format for each dataset.\n  - Handle challenges such as:\n    - Pagination\n    - Missing tags\n    - User-agent headers for mimicking a browser request.\n\n### 2. **Database Import Program**  \n- **Goal**: Import the CSV files into a **SQLite** database.\n- **Steps**:\n  - Create a program that imports each CSV as a separate table in the database.\n  - Ensure proper data types (numeric, date, etc.) during the import.\n  - Check for errors during the import process.\n\n### 3. **Database Query Program**  \n- **Goal**: Query the database via the command line.\n- **Steps**:\n  - Allow users to run queries, including at least **joins** (e.g., combining player stats with event data).\n  - Ensure the program can handle flexible querying, allowing for filtering by year, event, or player statistics.\n  - Handle errors and display results appropriately.\n\n### 4. **Dashboard Program**  \n- **Goal**: Build an interactive dashboard using **Streamlit** or **Dash**.\n- **Steps**:\n  - Display insights from the data using at least three visualizations.\n  - Implement interactive features like:\n    - Dropdowns to select years or event categories.\n    - Sliders to adjust the data view.\n  - Dynamically update the visualizations based on user input.\n  - Deploy the dashboard on **Render** or **Streamlit.io** for public access.\n\nAll four programs will be included in a new **GitHub** repository, and the dashboard will be deployed for public access.  This github repo should be separate from any of the other ones created for the class.\n\n## Data Sources\n\nStudents will scrape data from the **[Major League Baseball History Site](https://www.baseball-almanac.com/yearmenu.shtml)**. This site contains historical data such as notable events, player statistics, and achievements year by year.\n\n---\n\n## **Rubric for Lesson 14 - Web Scraping and Dashboard Project**\n\n### **Web Scraping**\n- Uses **Selenium** to retrieve data from the web.\n- Handles common scraping challenges like missing tags, pagination, and user-agent headers.\n- Saves raw data as a **CSV**.\n- Avoids scraping duplication or redundant requests.\n\n### **Data Cleaning & Transformation**\n- Loads raw data into a **Pandas DataFrame**.\n- Cleans missing, duplicate, or malformed entries effectively.\n- Applies appropriate transformations, groupings, or filters.\n- Shows before/after stages of cleaning or reshaping.\n\n### **Data Visualization**\n- Includes at least three visualizations using **Streamlit** or **Dash**.\n- Visuals are relevant, well-labeled, and support the data story.\n- User interactions such as dropdowns or sliders are implemented.\n- Visualizations respond correctly to user input or filters.\n\n### **Dashboard / App Functionality**\n- Built with **Streamlit** or **Dash** to display data and insights.\n- Features clean layout and responsive components.\n- Allows users to explore different aspects of the data.\n- Provides clear titles, instructions, and descriptions for user guidance.\n\n### **Code Quality & Documentation**\n- Code is well-organized and split into logical sections or functions.\n- Inline comments or markdown cells explain major steps or choices.\n- All dependencies are listed and environment setup is reproducible.\n- Comments or markdown cells explain logic.\n- **README.md** includes summary, setup steps, and a screenshot.\n\n---\n\n## **Submission Instructions:**\nWhen you are ready to submit your Kaggle and Webscraping projects, use the [link for the final project submission form](https://airtable.com/appoSRJMlXH9KvE6w/shrthD4fozy4UI21I?prefill_Lessons=Python%20100%20v1:%20Lesson%2015%20-%20Project%20Completion%20and%20Presentations).\n\n1. **Submit the Github Link**: Submit the link to your GitHub repository with all code files, data files, and the **README.md**.\n2. **Final Project Presentation**: Provide a brief explanation of your dashboard functionality and insights during the presentation. Submit your video link in the form.\n\n",
        "codeExample": "",
        "_id": "68796c79a4ac7ef2a17a61a6"
      }
    ],
    "submissionInstructions": "Please submit on time",
    "checklist": [],
    "checkForUnderstanding": []
  },
  "subsections": [
    {
      "subsectionOrder": 1,
      "title": "Lesson 14",
      "content": "# Lesson 14 — Web Scraping and Dashboard Project\n\n## Lesson Overview\nIn this lesson, students will create **four programs** that retrieve baseball history data from the **Major League Baseball History** website and display the results in an interactive dashboard. The project will involve **Selenium** for web scraping, data cleaning, transforming the data into a structured format, storing it in a SQLite database, querying it via command line, and presenting it using **Streamlit** or **Dash** with interactive visualizations.\n\n**Learning Objective:**  \nBy completing this project, students will:\n- Use **Selenium** to scrape data from a website.\n- Clean and transform the raw data into a structured format.\n- Store the data in a SQLite database, with each CSV file as a separate table.\n- Query the database using **joins** via command line.\n- Build an interactive dashboard using **Streamlit** or **Dash** to display the insights.\n\n**Projects:**\n1. **Web Scraping Program**: Scrape data from the [Major League Baseball History](https://www.baseball-almanac.com/yearmenu.shtml) website, assemble it into DataFrames, and store the data as several CSV files.\n2. **Database Import Program**: Import the CSV files into a **SQLite** database, with each CSV file as a separate table.\n3. **Database Query Program**: Create a command-line program to query the database using **joins** (e.g., joining player statistics with event data).\n4. **Dashboard Program**: Build a dashboard using **Streamlit** or **Dash** to visualize the data, allowing interactivity for users to explore the data.\n\nAll four programs will be included in the **GitHub** repository, and the dashboard will be deployed for public access.\n\n## Data Sources\n\nStudents will scrape data from the **[Major League Baseball History Site](https://www.baseball-almanac.com/yearmenu.shtml)**, which includes historical baseball data such as notable events, player statistics, and achievements year by year.\n",
      "videoUrl": "",
      "codeExamples": [],
      "externalLinks": [],
      "quizzes": [],
      "_id": "68796c79a4ac7ef2a17a61a5"
    }
  ]
}