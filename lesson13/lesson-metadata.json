{
  "id": "68796c79a4ac7ef2a17a61a0",
  "lessonNumber": 13,
  "title": "Lesson 13 — Capstone Project - Kaggle Dataset Project",
  "status": "pending",
  "assignment": {
    "title": "Assignment for Lesson 13",
    "objective": "No objective specified",
    "expectedCapabilities": [],
    "instructions": [],
    "tasks": [
      {
        "taskNumber": 1,
        "title": "Task 1",
        "description": "# Capstone Project - Kaggle Data Pipeline\n\nSelect one of the following datasets and perform cleaning, aggregation, analysis and visualization, illustrating at least 3 insights gleaned from the data.  Optionally, students can propose an alternative dataset and set of metrics to their CIL by the end of week 13.  The projects are intentionally open-ended in that it is up to the student to decide what they learn from the data, and how it is documented and visualized.\n\n## **Global Superstore**\n\nThe [Global Superstore](https://www.kaggle.com/datasets/anandaramg/global-superstore/data) is a collection of sales data from a large retailer.  The goal is to find, document and visualize at least three business insights from the data.\n\n1. **Data Conversion and Cleaning**\n\nLoad the data into a ``DataFrame`` using the appropriate delimiter.  Evaluate data quality, looking for issues such as duplicates, null values, and mixed formats.  If these are found, decide whether they signify an issue which needs to be corrected.  Some potential cleaning tasks include:\n\n* Convert dates to ``datetime``\n* Strip whitespace\n* Unify abbreviations such as state names\n* Convert numbers to ``float``\n\nDocument the data cleaning performed in a Markdown block\n\n2. **Feature Engineering**\n\nCreate at least three additional columns which can be used to derive insights from the data.  For example:\n\n* Columns for just ``year`` and ``month`\n* Derive ``Gross Margin`` from ``Profit`` and ``Sales``\n* Discretize discounts into buckets such as ``none``, ``low``, ``medium``, ``high``\n\n3. **Aggregation**\n\nPerform at least three aggregations to help drive insights.  For example:\n\n* top/bottom 10 states by total profit\n* Category ``Gross Margin``\n* ``Customer Lifetime Value`` (``CLV``) - Total profit each customer has generated across all orders\n\n4. **Analyze, Document, and Visualize**\n\nCreate at least three plots and associated metrics to illustrate the insights found in the data.\nInclude ``Markdown`` sections which explain the graphs and analysis.\n\n## **TMDB 5000 Movie Dataset**\n\nThe [TMDB 5000 Movie Dataset](https://www.kaggle.com/datasets/tmdb/tmdb-movie-metadata?select=tmdb_5000_movies.csv) is a collection of movies with a variety of data about production along with ratings.  The goal is to find the factors which influence critical and financial success.\n\n1. **Data Conversion and Cleaning**\n\nLoad the data into a ``DataFrame``.  Evaluate data quality, looking for issues such as duplicates, null values, composite data which could be expanded into additional columns, and mixed formats.  If these are found, decide whether they signify an issue which needs to be corrected.  Some potential cleaning tasks include:\n\n* Convert dates to ``datetime``, year vs date consistency\n* Handle null values if necessary\n* Numeric conversion e.g. ``revenue`` to ``float``\n* Expand/convert ``json`` formated columns, e'g' ``genres``, ``keywords``, ``production companies``\n* This is an explanation of the [format](https://www.kaggle.com/code/sohier/tmdb-format-introduction)\n\nDocument the data cleaning performed in a Markdown block\n\n2. **Feature Engineering**\n\nCreate at least three additional columns which can be used to derive insights from the data.  For example:\n\n* Can be satisfied by expanding ``json`` columns\n* Derive ``Gross Margin`` from ``Profit`` and ``Sales``\n* Discretize discounts into buckets such as ``none``, ``low``, ``medium``, ``high``\n\n3. **Aggregation**\n\nPerform at least three aggregations to help drive insights.  For example:\n\n* top/bottom 10 directors/actors/studios by rating(average vote)/revenue/budget/profit\n* return on investment profit vs budget\n* Discretize budgets\n\n4. **Analyze, Document, and Visualize**\n\nCreate at least three plots and associated metrics to illustrate the insights found in the data.\nInclude ``Markdown`` sections which explain the graphs and analysis.  Examples:\n\n* Best ``genres`` for a given success metric\n* Impact of particular actors and directors on success metrics\n* Genre popularity derived from vote count\n\n## **Life Expectancy (WHO)**\n\nThe [Life Expectancy (WHO)](https://www.kaggle.com/datasets/kumarajarshi/life-expectancy-who/data) is a collection of data by country and year with a variety of factors which may impact health.  The goal is to find, document and visualize at least three insights into the factors which influence longevity.\n\n1. **Data Conversion and Cleaning**\n\nLoad the data into a ``DataFrame``.  Evaluate data quality, looking for issues such as duplicates, null values, and mixed formats.  If these are found, decide whether they signify an issue which needs to be corrected.  Some potential cleaning tasks include:\n\n* Missing values as blanks or zeros\n* Conversion to numeric types\n* Unify country names, some appear in different forms\n* Outliers\n\nDocument the data cleaning performed in a Markdown block\n\n2. **Feature Engineering**\n\nCreate at least three additional columns which can be used to derive insights from the data.  For example:\n\n* Adult vs infant mortality\n* Discretize factors such as GDP\n* Combined vaccination rate\n\n3. **Aggregation**\n\nPerform at least three aggregations to help drive insights.  For example:\n\n* Global life-expectancy trend\n* Trends for developing vs developed\n* Life expectancy deciles (10 buckets) vs various driving factors\n\n4. **Analyze, Document, and Visualize**\n\nCreate at least three plots and associated metrics to illustrate the insights found in the data.\nInclude ``Markdown`` sections which explain the graphs and analysis.\n\n## **Seattle Airbnb Open Data (2016)**\n\nThe [Seattle Airbnb Open Data (2016)](https://www.kaggle.com/datasets/airbnb/seattle/data?select=listings.csv) is a collection of data on Airbnb listing in Seattle.  The goal is to find, document and visualize at least three insights into the factors which influence listing success, or conclusion which can be drawn about Seattle such as desirability or affluence for particular neighborhoods.\n\n1. **Data Conversion and Cleaning**\n\nLoad the data into a ``DataFrame``.  Evaluate data quality, looking for issues such as duplicates, null values, and mixed formats.  If these are found, decide whether they signify an issue which needs to be corrected.  Some potential cleaning tasks include:\n\n* Columns with a large percentage of nulls\n* Numeric formatting and conversion\n* Conversion of ``json``\n* Text with html tags\n\nDocument the data cleaning performed in a Markdown block\n\n2. **Feature Engineering**\n\nCreate at least three additional columns which can be used to derive insights from the data.  For example:\n\n* Derived from ``json`` columns\n* revenue per month\n* Simplified room type\n* Availability ratio\n\n3. **Aggregation**\n\nPerform at least three aggregations to help drive insights.  For example:\n\n* Median price by neighbourhood & simplified room type\n* Review score vs. amenity count\n* Monthly availability\n* 10 most/least affluent neighborhoods and associated profitability\n\n4. **Analyze, Document, and Visualize**\n\nCreate at least three plots and associated metrics to illustrate the insights found in the data.\nInclude ``Markdown`` sections which explain the graphs and analysis.\n\n\n## Kaggle Project Rubric\n\n* **General Code Quality**\n\n    * [ ]  Code demonstrates a strong understanding of Python basics. \n    * [ ]  Code is well organized and documented with comments.\n    * [ ]  Functions are used to structure and organize the code.\n\n* **File Handling and Data Loading**\n\n    * [ ]  Data is loaded from appropriate file formats (CSV, JSON, etc.) using Pandas.\n    * [ ]  File paths and loading procedures are clearly defined and handled robustly.\n    * [ ]  Demonstrates effective use of Pandas `read_csv()`, `read_json()`, or similar functions.\n    * [ ]  Uses `head()`, `tail()`, and `info()` effectively to preview and inspect the data.\n\n* **Data Wrangling and Transformation**\n\n    * [ ]  Demonstrates proficiency in using Pandas for data selection, filtering, and transformation.\n    * [ ]  Implements advanced data manipulation techniques, including indexing, slicing, and data type conversion.\n    * [ ]  Handles missing data effectively using `dropna()` or `fillna()` with appropriate strategies.\n    * [ ]  Identifies and removes duplicate records if necessary using Pandas.\n    * [ ]  Code is efficient, well-documented, and follows Pandas best practices.\n    * [ ]  At least three extracted features\n\n* **Data Aggregation**\n\n    * [ ]  Uses Pandas `groupby()` function effectively to aggregate data and gain insights.\n    * [ ]  Applies a variety of aggregation functions (e.g., `sum()`, `mean()`, `count()`, `min()`, `max()`) to analyze grouped data.\n    * [ ]  Clearly presents and interprets the results of data aggregation.\n    * [ ]  At least 3 aggregations\n\n* **Visualization Quality**\n\n    * [ ]  Creates multiple (3+) high-quality, informative, and visually appealing visualizations using appropriate libraries (e.g., Matplotlib, Seaborn, Plotly).\n    * [ ]  Visualizations are clear, concise, and easy to understand, with appropriate titles, labels, legends, and color schemes.\n    * [ ]  Demonstrates strong understanding of design principles.\n    * [ ]  Provides clear explanations of the insights conveyed by each visualization.\n\n* **Chart Types and Interpretation**\n\n    * [ ]  Uses a diverse range of chart types (e.g., scatter plots, bar charts, histograms, box plots, heatmaps) to provide a comprehensive view of the data.\n    * [ ]  Demonstrates a clear understanding of the strengths and weaknesses of each chart type and selects them strategically.\n    * [ ]  Provides insightful interpretations of the visualizations, connecting them to the data analysis and the problem domain.\n\n* **Dataset and Feature Understanding**\n\n    * [ ]  Uses a dataset that is appropriate for the analysis.\n    * [ ]  Demonstrates a clear understanding of the dataset's characteristics, limitations, and potential biases.\n    * [ ]  Selects and uses a sufficient number of relevant features to support a meaningful analysis.\n\n* **Conclusions and Insights**\n\n    * [ ]  Provides a clear, concise, and insightful summary of the project's key findings and conclusions.\n    * [ ]  Connects the findings to the original problem or question and discusses their implications.\n    * [ ]  Identifies potential limitations of the analysis.\n    * [ ]  Demonstrates a strong understanding of the data's story and effectively communicates it.\n    * [ ]  At least 3 conclusions supported by charts and text.\n\n* **Reproducibility**\n\n    * [ ]  Provides a well-organized and clearly documented notebook or script that allows others to easily reproduce the entire analysis.\n    * [ ]  All dependencies are clearly specified.\n\nWhen you are ready to submit your Kaggle and Web Scraping projects, use the [final project submission form](https://airtable.com/appoSRJMlXH9KvE6w/shrthD4fozy4UI21I?prefill_Lessons=Python%20100%20v1:%20Lesson%2015%20-%20Project%20Completion%20and%20Presentations).\n",
        "codeExample": "",
        "_id": "68796c79a4ac7ef2a17a61a2"
      }
    ],
    "submissionInstructions": "Please submit on time",
    "checklist": [],
    "checkForUnderstanding": []
  },
  "subsections": [
    {
      "subsectionOrder": 1,
      "title": "Lesson 13",
      "content": "# Lesson 13 — Capstone Project - Kaggle Dataset Project\n\n## Lesson Overivew\nIn the final 3 weeks, student will create two capstone projects to demonstrate the skills learned in the class in a portfolio suitable for presentation.\n\n**Learning Objective:** \nBy completing this project, students will demonstrate their ability to load, clean, analyze, and visualize real-world data using Python. They will apply core programming skills, data wrangling techniques, and effective visualization practices to extract and communicate insights. Students will also build fluency with project structure, code clarity, and reproducibility standards in preparation for real-world data work.\n\n**Projects**\n1. A data pipeline which demonstrates cleaning, aggregation, analysis and visualization in a Kaggle Notebook\n2. A local project published in github which demonstrates web scraping, cleaning, analysis and presentation using plotly and Dash or Streamlit\n\n## Data Sources\n\n### Kaggle Data Pipeline\n\n  The student can select one of four curated datasets listed below or one of their own choosing with CIL approval.\n\n  **Global Superstore**\n\n    Find business insights from sales data at a global superstore to drive business decisions.\n\n  **TMDB 5000 Movie Dataset**\n\n    Find the factors which influence movie performance to find factors which influence success.\n\n  **Life Expectancy (WHO)**\n\n    Find the factors which influence life expectancy.\n\n  **Seattle Airbnb Open Data**\n\n    Find the factors which influence listing performance.\n\n\n### Web Scraping Dashboard\n\nStudents will scrape the [Major League Baseball History site](https://www.baseball-almanac.com/yearmenu.shtml) and display results in a dashboard.  Details will be provided in lesson/assignment 14.\n\n**Kaggle Data Pipeline Project details are provided in the Assignments Section**\n",
      "videoUrl": "",
      "codeExamples": [],
      "externalLinks": [],
      "quizzes": [],
      "_id": "68796c79a4ac7ef2a17a61a1"
    }
  ]
}